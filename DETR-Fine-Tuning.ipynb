{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.datasets import CocoDetection\n",
    "from transformers import DetrImageProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import DetrForObjectDetection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeatbeltCocoDataset(CocoDetection):\n",
    "    \n",
    "    def __init__(self, img_folder, processor):\n",
    "        ann_file = os.path.join(img_folder, \"_annotations.coco.json\")\n",
    "        super(SeatbeltCocoDataset, self).__init__(img_folder, ann_file)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        # feel free to add data augmentation here before passing them to the next step\n",
    "        img, target = super(SeatbeltCocoDataset, self).__getitem__(idx)\n",
    "        \n",
    "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        encoding = self.processor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "        target = encoding[\"labels\"][0] # remove batch dimension\n",
    "\n",
    "        return pixel_values, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "train_dataset = SeatbeltCocoDataset(img_folder='./data/train', processor=processor)\n",
    "val_dataset = SeatbeltCocoDataset(img_folder='./data//valid', processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://github.com/woctezuma/finetune-detr/blob/master/finetune_detr.ipynb\n",
    "image_ids = train_dataset.coco.getImgIds()\n",
    "\n",
    "# let's pick a random image\n",
    "image_id = image_ids[np.random.randint(0, len(image_ids))]\n",
    "print('Image nÂ°{}'.format(image_id))\n",
    "image = train_dataset.coco.loadImgs(image_id)[0]\n",
    "image = Image.open(os.path.join('./data/train', image['file_name']))\n",
    "\n",
    "annotations = train_dataset.coco.imgToAnns[image_id]\n",
    "draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "\n",
    "cats = train_dataset.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}\n",
    "\n",
    "for annotation in annotations:\n",
    "  box = annotation['bbox']\n",
    "  class_idx = annotation['category_id']\n",
    "  x,y,w,h = tuple(box)\n",
    "  draw.rectangle((x,y,x+w,y+h), outline='red', width=1)\n",
    "  draw.text((x, y), id2label[class_idx], fill='white')\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values, target = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  \n",
    "  pixel_values = [item[0] for item in batch]\n",
    "  encoding = processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "  labels = [item[1] for item in batch]\n",
    "  batch = {}\n",
    "  batch['pixel_values'] = encoding['pixel_values']\n",
    "  batch['pixel_mask'] = encoding['pixel_mask']\n",
    "  batch['labels'] = labels\n",
    "  \n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "batch.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model\n",
    "lr = 0.0001\n",
    "lr_backbone = 0.00001\n",
    "weight_decay = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\",\n",
    "                                                             revision=\"no_timm\", \n",
    "                                                             num_labels=len(id2label),\n",
    "                                                             ignore_mismatched_sizes=True).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        pixel_mask = batch[\"pixel_mask\"].to(device)\n",
    "        labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss  # Access the loss from the outputs object\n",
    "\n",
    "        # loss_dict = outputs.loss_dict\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in val_dataloader:\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            pixel_mask = batch[\"pixel_mask\"].to(device)\n",
    "            labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "            loss = outputs.loss  # Access the loss from the outputs object\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {total_val_loss / len(val_dataloader)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './saved_model.pth'\n",
    "torch.save(model, path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SeatbeltCocoDataset(img_folder='./data/test', processor=processor)\n",
    "pixel_values, target = test_dataset[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = pixel_values.unsqueeze(0).to(device)\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = model(pixel_values=pixel_values, pixel_mask=None)\n",
    "    \n",
    "print(\"Outputs:\", outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "def plot_results(pil_img, scores, labels, boxes):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    \n",
    "    for score, label, (xmin, ymin, xmax, ymax),c  in zip(scores.tolist(), labels.tolist(), boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        text = f'{id2label[label]}: {score:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image based on ID\n",
    "image_id = target['image_id'].item()\n",
    "image = test_dataset.coco.loadImgs(image_id)[0]\n",
    "image = Image.open(os.path.join('./data/test', image['file_name']))\n",
    "\n",
    "# postprocess model outputs\n",
    "width, height = image.size\n",
    "postprocessed_outputs = processor.post_process_object_detection(outputs,\n",
    "                                                                target_sizes=[(height, width)],\n",
    "                                                                threshold=0.7)\n",
    "\n",
    "results = postprocessed_outputs[0]\n",
    "plot_results(image, results['scores'], results['labels'], results['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DETR-Venv",
   "language": "python",
   "name": "detr-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
